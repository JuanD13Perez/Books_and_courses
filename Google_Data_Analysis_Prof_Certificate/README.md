# Google Data Analysis Professional Capstone Project
- [English Version](#English-Version) 
- [Spanish Version](#Spanish-Version)

## Spanish Version

### **Problema de negocio** üìù
Este proyecto hace parte del certificado profesional de Google en an√°lisis de datos.
Usamos el dataset de la marca _FitBit_, disponible v√≠a [**Kaggle**](https://www.kaggle.com/datasets/arashnic/fitbit), que corresponden a registros de un dispositvo (Smartwatch) con informaci√≥n de consumo de calor√≠as, n√∫mero de pasos, etc.
En este caso, asumimos que trabajamos para Bellabeat, una competidora de FitBit. Nuestra tarea consiste en **identificar patrones de consumo en los usuarios de los productos FitBit** que puedan apoyar la estrategia de Marketing de la compa√±√≠a.

| ![Banner](FitBit_Banner.png) |
|:--:|
| <b>Image Credits - Banner - https://www.kaggle.com/datasets/arashnic/fitbit  </b>|


### **Highlights** 
Este proyecto se hizo utlizando python 3.10.5 en un entorno de Jupyter Notebooks. Consta de tres secciones principales:
- [**Ap√©ndice**](#Ap√©ndice)
- [**Data Cleaning**](#Data-Cleaning)
- [**Data Analysis**](#Data-Analysis)
### Ap√©ndice
Aqu√≠ manipulamos desde el sistema el [dataset](https://www.kaggle.com/datasets/arashnic/fitbit) con el fin de obtener todos los archivos csv como pandas dataframes. Nos dimos cuenta que la estructura de datos es similar a la de una base relacional y que ciertos dataframes son meramente agregaciones de los datos minuto a minuto. Decidimos trabajar √∫nicamente con √©stos dataframes 'ra√≠z'. 

Tenemos informaci√≥n por minuto de:
* Calor√≠as
* Intensidad de ejercicio
* METs
* N√∫mero de pasos
* Ritmo card√≠aco
Informaci√≥n diaria de:
* Sue√±o
* Peso

### Data Cleaning:

0. Restringimos a 19 usuarios de los 25 originalmente disponibles. Ya que 6 usuarios ten√≠an la informaci√≥n muy incompleta (no alcanzaban a cubrir un mes de datos).
1. Restringimos el rango de fechas desde 2016-04-12 hasta 2016-05-11 minuto a minuto. Esto para tener informaci√≥n de tiempo comparable para los distintos usuarios. Aqu√≠ se usaron todos los datos de Calor√≠as, Intensidades, METs y Steps.
2. Interpolamos usando statistical matching registros faltantes de sue√±o y a√±adimos ruido normal para completar registros inexistentes de algunos usuarios a fin de evitar sesgo y respetar la distribuci√≥n original. Datos son diarios para el mismo per√≠odo. Ver nota.
3. Interpolamos datos faltantes de ritmo card√≠aco usando f√≥rmula que relaciona METs., bas√°ndonos en un m√©todo lineal robusto expuesto en [ELSEVIER- Int J Cardiol Heart Vasc.](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6003065/). Adem√°s a√±adimos registros inexistentes con la misma f√≥rmula + resampleo aleatorio de los registros existentes para tener datos que respeten la distribuci√≥n original de los Id's presentes. Ver nota.
4. Desestimamos el uso de los datos de Peso al ser la mayor√≠a subjetivamente ingresados por los usuarios. Esto los hace vulnerables a sesgo o malas mediciones. Adem√°s cont√°bamos con muy pocos registros completos (apenas 2 de 19).

Nota: Obviamente entendemos que estos datos son ficticios. En un caso real el problema de la falta de registros deber√≠a tratarse con los stakeholders. El √∫nico prop√≥sito aqu√≠ es demostrar habilidad en el uso de m√©todos estad√≠sticos. El muestreo aleatorio puede llevar a un conjunto de datos de mayor calidad, reduciendo el sesgo que √∫nicamente usando la peque√±a proporci√≥n incompleta de datos disponibles. Sin embargo, no defendemos aqu√≠ que sea una buena pr√°ctica hacer este tipo de interpolaciones.

### Data Analysis: 



#### Conclusiones


### Requerimientos
 * Descargar el dataset desde [Kaggle Fit Bit](https://www.kaggle.com/datasets/arashnic/fitbit)
 * Se us√≥ Python 3.10.5 y las siguientes librer√≠as:
 ```
pandas
numpy 
from scipy.stats import trim_mean
matplotlib.pyplot
seaborn
 ````

## English Version